{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.ldamulticore import LdaMulticore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import multiprocessing\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trainning code\n",
    "\n",
    "dataset = 'arxiv-metadata-oai-snapshot.json'\n",
    "\n",
    "chunk_size = 1000\n",
    "chunks =[]\n",
    "# chia nhỏ dữ liệu rồi đọc để tránh Memory Error\n",
    "for chunk in pd.read_json(dataset,lines = True,chunksize = chunk_size):\n",
    "    chunks.append(chunk)\n",
    "\n",
    "# nối list các dataframe lại thành 1 dataframe duy nhất\n",
    "df = pd.concat(chunks, ignore_index=True)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lược bỏ các cột có thông tin không cần thiết để giảm lượng dữ liệu cần tương tác\n",
    "df.drop(columns=['submitter','authors','comments','journal-ref','report-no','license','versions'],inplace = True)\n",
    "# loại bỏ các điểm dữ liệu có không có thông tin\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sắp xếp lại dữ liệu theo thứ tự thời gian giảm dần (để khi blind_search_engine.search() thì kết quả đầu tiên sẽ là các bài báo mới nhất)\n",
    "from datetime import datetime\n",
    "df['update_date'] = pd.to_datetime(df['update_date'])\n",
    "df_sorted = df.sort_values(by='update_date', ascending=False)\n",
    "\n",
    "#lưu dữ liệu thành file csv\n",
    "df_sorted.to_csv('new_sorted_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load df from saved_file \n",
    "chunks =[]\n",
    "\n",
    "for chunk in pd.read_csv('sorted_df.csv', chunksize=10000):\n",
    "    chunks.append(chunk)\n",
    "\n",
    "load_df = pd.concat(chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tách phần abstract ra riêng để dễ dàng thao tác, huấn luyện\n",
    "abstracts = load_df['abstract']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\japan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\japan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\japan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#downloading các tệp cần thiết cho việc tokenize, loại bỏ stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# hàm lemmatize, tokenize và loại bỏ stopwords trong dữ liệu \n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    words = word_tokenize(text)\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatize, tokenize và loại bỏ stopwords đối với abstracts \n",
    "abstracts = [preprocess(text) for text in abstracts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hàm tự viết để lưu file \n",
    "\n",
    "def write_file(file_name: str,content):\n",
    "    f = open(file_name,'w')\n",
    "    for chunk in content:\n",
    "        f.write(str(chunk) + '\\n')\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def read_file(file_name:str):\n",
    "    import ast\n",
    "    contents = []\n",
    "    with open(file_name, 'r') as file:\n",
    "        for line in file:\n",
    "            chunk = ast.literal_eval(line)\n",
    "            contents.append(chunk)\n",
    "    return contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lưu file abstract để tránh phải chạy lại khi xảy ra lỗi\n",
    "write_file('new_abstracts.txt',abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# đọc abstracts từ file đã lưu\n",
    "abs = read_file('new_abstracts.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#tạo dictionary từ dữ liệu đã qua xử lý \n",
    "dictionary = Dictionary(abs)\n",
    "\n",
    "#loại bỏ các xuất hiện trong ít hơn 5 văn bản và các từ xuất hiện trong 50% các văn bản\n",
    "dictionary.filter_extremes(no_below=5, no_above=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Dictionary saved to 'dictionary.dict'\n"
     ]
    }
   ],
   "source": [
    "#lưu dictionary \n",
    "dictionary.save('dictionary.dict')\n",
    "print(\"New Dictionary saved to 'dictionary.dict'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary loaded from 'dictionary.dict'\n"
     ]
    }
   ],
   "source": [
    "#load dictionary\n",
    "dictionary = Dictionary.load('dictionary.dict')\n",
    "print(\"Dictionary loaded from 'dictionary.dict'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#tạo bag of words  với mỗi một abstract ta có\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in abs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New BoW corpus saved to 'bow_corpus.pkl'\n"
     ]
    }
   ],
   "source": [
    "#Save  BoW\n",
    "\n",
    "import pickle\n",
    "with open('bow_corpus.pkl', 'wb') as f:\n",
    "    pickle.dump(bow_corpus, f)\n",
    "\n",
    "print(\"New BoW corpus saved to 'bow_corpus.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW corpus loaded from 'bow_corpus.pkl'\n"
     ]
    }
   ],
   "source": [
    "#Load Bow\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('bow_corpus.pkl', 'rb') as f:\n",
    "    bow_corpus = pickle.load(f)\n",
    "\n",
    "print(\"BoW corpus loaded from 'bow_corpus.pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chạy với 11 core và hoàn thành sau 2 tiếng 10p\n",
    "\n",
    "import multiprocessing\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "# đặt số lượng chủ đề muốn LDA tìm ra là 80\n",
    "# tại sao lại là 80? Tại vì nguồn của dữ liệu là trang Axvir có 8 chủ đề bài báo khoa học, nên đặt là 80 :D\n",
    "# đặt cao quá thì ko chạy được (đã thử 400 và 200)\n",
    "num_topics = 80\n",
    "\n",
    "# passes là số lần toàn bộ tập dữ liệu sẽ được đi qua trong quá trình huấn luyện mô hình, ta đặt nó là 20\n",
    "# tại sao? vì mặc định trong mô hình là 20 :(. Viết cho vui thôi chứ để cao quá máy chạy đến đời chắt\n",
    "passes = 20\n",
    "\n",
    "# thấy mọi người khuyên khi sử dụng song song thì nên có dòng \"if __name__ == \"__main__\"\" để tránh bị vòng lặp vô hạn, file tự call đến chính nó vô hạn lần\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    num_workers = multiprocessing.cpu_count() - 1  # Use all CPUs except one\n",
    "\n",
    "    # sử dụng LDA multicore để chạy song song dữ liệu để chạy nhanh hơn\n",
    "    lda_model = LdaMulticore(\n",
    "        corpus=bow_corpus,\n",
    "        id2word=dictionary,\n",
    "        num_topics=num_topics,\n",
    "        passes=passes,\n",
    "        workers=num_workers\n",
    "    )\n",
    "\n",
    "#show topics\n",
    "topics = lda_model.show_topics(num_topics=num_topics, num_words=10, log=False, formatted=False)\n",
    "\n",
    "\n",
    "for topic_id, topic in topics:\n",
    "    print(\"Topic: {}\".format(topic_id))\n",
    "    print(\"Words: {}\".format([word for word, _ in topic]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA model saved to lda_model_80.model\n"
     ]
    }
   ],
   "source": [
    "#save thông tin mô hình sau khi được chạy xong để không phải chạy lại lần nữa nếu vẫn sữ dụng thông tin cũ\n",
    "lda_model.save('new_lda_model.model')\n",
    "print(f\"LDA model saved to lda_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA model loaded\n",
      "Topic 0: 0.030*\"model\" + 0.027*\"pattern\" + 0.022*\"cell\" + 0.015*\"population\" + 0.009*\"protein\"\n",
      "Topic 1: 0.195*\"phase\" + 0.111*\"transition\" + 0.044*\"critical\" + 0.025*\"temperature\" + 0.024*\"model\"\n",
      "Topic 2: 0.040*\"structure\" + 0.028*\"molecule\" + 0.023*\"molecular\" + 0.021*\"calculation\" + 0.021*\"electronic\"\n",
      "Topic 3: 0.037*\"solar\" + 0.021*\"flux\" + 0.014*\"flare\" + 0.012*\"region\" + 0.010*\"cycle\"\n",
      "Topic 4: 0.251*\"spectrum\" + 0.104*\"power\" + 0.076*\"spectral\" + 0.061*\"perturbation\" + 0.056*\"fluctuation\"\n",
      "Topic 5: 0.114*\"trajectory\" + 0.083*\"glass\" + 0.051*\"overlap\" + 0.043*\"slope\" + 0.024*\"replica\"\n",
      "Topic 6: 0.045*\"star\" + 0.031*\"mass\" + 0.028*\"disk\" + 0.018*\"formation\" + 0.017*\"model\"\n",
      "Topic 7: 0.025*\"outgoing\" + 0.023*\"oblique\" + 0.017*\"penrose\" + 0.014*\"unfolding\" + 0.011*\"dt\"\n",
      "Topic 8: 0.023*\"particle\" + 0.014*\"surface\" + 0.012*\"liquid\" + 0.012*\"simulation\" + 0.012*\"fluid\"\n",
      "Topic 9: 0.023*\"redshift\" + 0.022*\"survey\" + 0.012*\"data\" + 0.012*\"cosmic\" + 0.012*\"galaxy\"\n",
      "Topic 10: 0.298*\"'\" + 0.150*\"e\" + 0.132*\"h\" + 0.087*\"`\" + 0.026*\"de\"\n",
      "Topic 11: 0.111*\"method\" + 0.022*\"approach\" + 0.020*\"numerical\" + 0.020*\"scheme\" + 0.016*\"simulation\"\n",
      "Topic 12: 0.054*\"optical\" + 0.040*\"photon\" + 0.028*\"laser\" + 0.026*\"light\" + 0.022*\"cavity\"\n",
      "Topic 13: 0.092*\"channel\" + 0.019*\"transmission\" + 0.017*\"communication\" + 0.015*\"capacity\" + 0.013*\"proposed\"\n",
      "Topic 14: 0.478*\"cluster\" + 0.046*\"branch\" + 0.040*\"globular\" + 0.024*\"member\" + 0.019*\"mirror\"\n",
      "Topic 15: 0.072*\"mode\" + 0.067*\"wave\" + 0.045*\"frequency\" + 0.023*\"oscillation\" + 0.015*\"resonance\"\n",
      "Topic 16: 0.058*\"planet\" + 0.017*\"planetary\" + 0.016*\"atmosphere\" + 0.012*\"water\" + 0.012*\"earth\"\n",
      "Topic 17: 0.123*\"]\" + 0.123*\"[\" + 0.048*\"et\" + 0.045*\"al\" + 0.029*\"abundance\"\n",
      "Topic 18: 0.052*\"curve\" + 0.050*\"supernova\" + 0.028*\"sn\" + 0.025*\"light\" + 0.025*\"white\"\n",
      "Topic 19: 0.050*\"system\" + 0.048*\"dynamic\" + 0.046*\"time\" + 0.018*\"process\" + 0.015*\"dynamical\"\n",
      "Topic 20: 0.232*\"}\" + 0.232*\"{\" + 0.044*\"r\" + 0.021*\"f\" + 0.021*\"\\mathcal\"\n",
      "Topic 21: 0.062*\"velocity\" + 0.040*\"force\" + 0.040*\"motion\" + 0.034*\"vortex\" + 0.029*\"domain\"\n",
      "Topic 22: 0.027*\"decay\" + 0.016*\"production\" + 0.014*\"collision\" + 0.012*\"quark\" + 0.010*\"gev\"\n",
      "Topic 23: 0.049*\"state\" + 0.039*\"interaction\" + 0.029*\"energy\" + 0.025*\"potential\" + 0.013*\"ground\"\n",
      "Topic 24: 0.071*\"problem\" + 0.061*\"algorithm\" + 0.022*\"optimal\" + 0.014*\"optimization\" + 0.011*\"solution\"\n",
      "Topic 25: 0.119*\"network\" + 0.028*\"neural\" + 0.026*\"classification\" + 0.026*\"input\" + 0.019*\"signal\"\n",
      "Topic 26: 0.045*\"theory\" + 0.029*\"string\" + 0.016*\"field\" + 0.016*\"dimension\" + 0.013*\"model\"\n",
      "Topic 27: 0.030*\"surface\" + 0.027*\"layer\" + 0.023*\"film\" + 0.016*\"graphene\" + 0.013*\"defect\"\n",
      "Topic 28: 0.052*\"ii\" + 0.041*\"agn\" + 0.034*\"quasar\" + 0.024*\"nucleus\" + 0.024*\"iii\"\n",
      "Topic 29: 0.018*\"role\" + 0.018*\"physic\" + 0.016*\"recent\" + 0.014*\"new\" + 0.013*\"review\"\n",
      "Topic 30: 0.029*\"measurement\" + 0.029*\"noise\" + 0.020*\"resolution\" + 0.011*\"sensitivity\" + 0.011*\"detector\"\n",
      "Topic 31: 0.020*\"user\" + 0.018*\"language\" + 0.015*\"information\" + 0.012*\"word\" + 0.012*\"content\"\n",
      "Topic 32: 0.032*\"code\" + 0.020*\"tool\" + 0.019*\"program\" + 0.014*\"present\" + 0.013*\"available\"\n",
      "Topic 33: 0.054*\"hole\" + 0.048*\"black\" + 0.021*\"gravity\" + 0.017*\"solution\" + 0.016*\"gravitational\"\n",
      "Topic 34: 0.035*\"theory\" + 0.034*\"field\" + 0.013*\"tensor\" + 0.012*\"term\" + 0.012*\"equation\"\n",
      "Topic 35: 0.129*\"quantum\" + 0.073*\"state\" + 0.023*\"entanglement\" + 0.019*\"system\" + 0.017*\"classical\"\n",
      "Topic 36: 0.051*\"symmetry\" + 0.029*\"gauge\" + 0.029*\"model\" + 0.026*\"mass\" + 0.022*\"chiral\"\n",
      "Topic 37: 0.042*\"protocol\" + 0.029*\"key\" + 0.024*\"attack\" + 0.023*\"security\" + 0.012*\"secure\"\n",
      "Topic 38: 0.197*\"depth\" + 0.044*\"texture\" + 0.039*\"penetration\" + 0.037*\"shallow\" + 0.027*\"vi\"\n",
      "Topic 39: 0.212*\"bound\" + 0.071*\"error\" + 0.068*\"lower\" + 0.061*\"upper\" + 0.036*\"code\"\n",
      "Topic 40: 0.107*\"''\" + 0.070*\"``\" + 0.038*\"--\" + 0.013*\"one\" + 0.012*\"'s\"\n",
      "Topic 41: 0.064*\"control\" + 0.023*\"system\" + 0.022*\"strategy\" + 0.021*\"agent\" + 0.019*\"game\"\n",
      "Topic 42: 0.079*\"pulsar\" + 0.057*\"time\" + 0.051*\"pulse\" + 0.032*\"delay\" + 0.029*\"period\"\n",
      "Topic 43: 0.030*\"learning\" + 0.025*\"model\" + 0.017*\"data\" + 0.017*\"method\" + 0.014*\"deep\"\n",
      "Topic 44: 0.032*\"scattering\" + 0.031*\"nuclear\" + 0.023*\"cross\" + 0.021*\"section\" + 0.020*\"energy\"\n",
      "Topic 45: 0.078*\"system\" + 0.046*\"binary\" + 0.042*\"orbit\" + 0.034*\"orbital\" + 0.028*\"stable\"\n",
      "Topic 46: 0.069*\"temperature\" + 0.031*\"thermal\" + 0.028*\"current\" + 0.027*\"transport\" + 0.021*\"effect\"\n",
      "Topic 47: 0.037*\"event\" + 0.032*\"signal\" + 0.029*\"detector\" + 0.027*\"detection\" + 0.022*\"search\"\n",
      "Topic 48: 0.030*\"device\" + 0.019*\"gate\" + 0.017*\"quantum\" + 0.014*\"application\" + 0.013*\"circuit\"\n",
      "Topic 49: 0.039*\"radio\" + 0.037*\"source\" + 0.028*\"jet\" + 0.028*\"emission\" + 0.020*\"burst\"\n",
      "Topic 50: 0.223*\"$\" + 0.219*\"}\" + 0.219*\"{\" + 0.030*\"^\" + 0.025*\"_\"\n",
      "Topic 51: 0.039*\"}\" + 0.039*\"{\" + 0.038*\"%\" + 0.037*\"=\" + 0.028*\"10^\"\n",
      "Topic 52: 0.179*\"anomalous\" + 0.162*\"anomaly\" + 0.074*\"trace\" + 0.046*\"monopole\" + 0.027*\"monopoles\"\n",
      "Topic 53: 0.021*\"temperature\" + 0.017*\"_2\" + 0.013*\"k\" + 0.010*\"superconducting\" + 0.009*\"state\"\n",
      "Topic 54: 0.082*\"function\" + 0.042*\"distribution\" + 0.029*\"correlation\" + 0.026*\"density\" + 0.025*\"model\"\n",
      "Topic 55: 0.273*\"-\" + 0.071*\">\" + 0.043*\"gamma\" + 0.040*\"ir\" + 0.040*\"alpha\"\n",
      "Topic 56: 0.086*\"galaxy\" + 0.022*\"mass\" + 0.018*\"halo\" + 0.012*\"formation\" + 0.011*\"stellar\"\n",
      "Topic 57: 0.052*\"figure\" + 0.032*\"h_2\" + 0.015*\"coo\" + 0.015*\"n=8\" + 0.014*\"concentrating\"\n",
      "Topic 58: 0.082*\"network\" + 0.062*\"graph\" + 0.034*\"node\" + 0.023*\"vertex\" + 0.022*\"link\"\n",
      "Topic 59: 0.039*\"model\" + 0.037*\"matter\" + 0.034*\"dark\" + 0.025*\"neutrino\" + 0.022*\"mass\"\n",
      "Topic 60: 0.090*\"n\" + 0.050*\"x\" + 0.035*\">\" + 0.034*\"<\" + 0.034*\"1\"\n",
      "Topic 61: 0.096*\"equation\" + 0.057*\"solution\" + 0.026*\"flow\" + 0.020*\"condition\" + 0.018*\"boundary\"\n",
      "Topic 62: 0.036*\"correction\" + 0.033*\"order\" + 0.022*\"expansion\" + 0.020*\"contribution\" + 0.019*\"amplitude\"\n",
      "Topic 63: 0.039*\"pl\" + 0.026*\"t-duality\" + 0.020*\"invisible\" + 0.018*\"n_h\" + 0.012*\"dyson-schwinger\"\n",
      "Topic 64: 0.069*\"model\" + 0.042*\"data\" + 0.029*\"parameter\" + 0.015*\"analysis\" + 0.012*\"estimate\"\n",
      "Topic 65: 0.052*\"star\" + 0.013*\"stellar\" + 0.010*\"object\" + 0.009*\"dwarf\" + 0.008*\"age\"\n",
      "Topic 66: 0.052*\"x-ray\" + 0.048*\"line\" + 0.030*\"emission\" + 0.023*\"spectrum\" + 0.017*\"source\"\n",
      "Topic 67: 0.030*\"scaling\" + 0.023*\"size\" + 0.023*\"random\" + 0.023*\"length\" + 0.018*\"exponent\"\n",
      "Topic 68: 0.023*\"system\" + 0.014*\"data\" + 0.014*\"memory\" + 0.013*\"performance\" + 0.010*\"resource\"\n",
      "Topic 69: 0.139*\"energy\" + 0.061*\"electron\" + 0.047*\"plasma\" + 0.046*\"ion\" + 0.031*\"beam\"\n",
      "Topic 70: 0.014*\"research\" + 0.012*\"data\" + 0.010*\"study\" + 0.007*\"paper\" + 0.006*\"analysis\"\n",
      "Topic 71: 0.093*\"image\" + 0.027*\"3d\" + 0.023*\"object\" + 0.023*\"map\" + 0.021*\"reconstruction\"\n",
      "Topic 72: 0.794*\"$\" + 0.009*\"\\alpha\" + 0.006*\"\\lambda\" + 0.006*\"\\sim\" + 0.006*\"\\gamma\"\n",
      "Topic 73: 0.028*\"state\" + 0.019*\"topological\" + 0.015*\"quantum\" + 0.015*\"system\" + 0.012*\"gap\"\n",
      "Topic 74: 0.237*\"field\" + 0.140*\"magnetic\" + 0.038*\"polarization\" + 0.028*\"electric\" + 0.015*\"moment\"\n",
      "Topic 75: 0.040*\"gas\" + 0.024*\"region\" + 0.021*\"dust\" + 0.019*\"cloud\" + 0.016*\"emission\"\n",
      "Topic 76: 0.022*\"group\" + 0.021*\"space\" + 0.016*\"algebra\" + 0.012*\"operator\" + 0.010*\"class\"\n",
      "Topic 77: 0.121*\"spin\" + 0.030*\"magnetic\" + 0.020*\"interaction\" + 0.020*\"coupling\" + 0.015*\"magnetization\"\n",
      "Topic 78: 0.173*\"sequence\" + 0.058*\"category\" + 0.043*\"base\" + 0.034*\"dna\" + 0.033*\"causal\"\n",
      "Topic 79: 0.167*\"ensemble\" + 0.067*\"canonical\" + 0.057*\"cascade\" + 0.026*\"1/3\" + 0.024*\"avalanche\"\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import LdaModel\n",
    "num_topics = 80\n",
    "\n",
    "#load LDA_model\n",
    "model_file = 'new_lda_model.model'\n",
    "loaded_lda_model = LdaModel.load(model_file)\n",
    "print(\"LDA model loaded\")\n",
    "\n",
    "#show topics từ model mới được load lên\n",
    "topics = loaded_lda_model.print_topics(num_topics=num_topics, num_words=5)\n",
    "for topic_id, topic in topics:\n",
    "    print(f\"Topic {topic_id}: {topic}\")\n",
    "\n",
    "lda_model = loaded_lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hàm tìm topics của 1 bag of words và lấy ra num chủ để có tỉ lệ cao nhất có xác suất không kém no_lower_than\n",
    "#bag of words này sẽ là 1 bow của abstract tìm chủ đề và xếp bài đó vào danh sách chủ để\n",
    "#bag of words này cũng sẽ là đầu vào, thông tin người dùng nhập vào thanh search để tìm kiếm\n",
    "def bow_to_topicid(bow,no_lower_than = 0.2,num=3):\n",
    "    p_topics = lda_model.get_document_topics(bow, minimum_probability = no_lower_than)\n",
    "    top = sorted(p_topics, key=lambda x: x[1], reverse=True)[:num]\n",
    "    return top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hàm tìm kiếm và trả về xác suất chủ đề của chuỗi kí tự so với dữ liệu của lda\n",
    "def search_to_topicid(search,no_lower_than = 0.2,num=3):\n",
    "    \n",
    "    preprocess_search = preprocess(search)\n",
    "    bow_search = dictionary.doc2bow(preprocess_search)\n",
    "    top = bow_to_topicid(bow_search,no_lower_than,num)\n",
    "    return top\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bước chuẩn bị để tạo một dictionary convert từ topic ra danh sách các bài báo thuộc chủ đề đó.\n",
    "id = df['id'].values\n",
    "id.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary để chuyển từ id của bài báo thành BoW của bài báo đó\n",
    "id_to_bow = {}\n",
    "for i in range(len(bow_corpus)):\n",
    "    id_to_bow[id[i]] = bow_corpus[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save id_to_bow\n",
    "import pickle \n",
    "\n",
    "with open('new_id_to_bow.pkl', 'wb') as f:\n",
    "    pickle.dump(id_to_bow, f)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load id_to_bow\n",
    "import pickle \n",
    "with open('new_id_to_bow.pkl', 'rb') as f:\n",
    "    loaded_id_to_bow = pickle.load(f)\n",
    "\n",
    "id_to_bow = loaded_id_to_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tạo topicid_to_ids, dictionary tra cứu từ topicid thành list các bài báo thuộc chủ đề đó\n",
    "\n",
    "values = [[] for _ in range(num_topics)]\n",
    "\n",
    "for id,bow in id_to_bow.items():\n",
    "    top = bow_to_topicid(bow)\n",
    "    for topic_id, v in top:\n",
    "        values[topic_id].append(id)\n",
    "\n",
    "topicid_to_ids = {}\n",
    "\n",
    "for i in range(len(values)):\n",
    "    topicid_to_ids[i] = values[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save topicid_to_ids\n",
    "import pickle \n",
    "\n",
    "with open('new_topicid_to_ids.pkl', 'wb') as f:\n",
    "    pickle.dump(topicid_to_ids, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load topicid_to_ids\n",
    "import pickle \n",
    "with open('new_topicid_to_ids.pkl', 'rb') as f:\n",
    "    topicid_to_ids = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# từ thông tin id của bài báo để lấy về các thông tin đầy đủ của bài báo\n",
    "\n",
    "def id_to_contents(id):\n",
    "    result = df.loc[df['id'] == id]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hàm trả về các bài báo có chủ đề liên quan đến chủ đề nhận được\n",
    "def search_results(top,num_result = 10, topics = 3):\n",
    "    results = df.iloc[:2]\n",
    "    results = results.drop([0,1])\n",
    "\n",
    "\n",
    "    \n",
    "    for topicid, prob in top[:topics]:\n",
    "        ids = topicid_to_ids[topicid]\n",
    "       \n",
    "        for id in ids:\n",
    "            next = id_to_contents(id)\n",
    "           \n",
    "            if num_result == 0 :\n",
    "                break\n",
    "            elif next is None:\n",
    "                break\n",
    "            else:\n",
    "                num_result = num_result -1\n",
    "            results = pd.concat([results, next],axis = 0)\n",
    "  \n",
    "\n",
    "    return results\n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hàm trả về các bài báo có chủ đề liên quan đến chuỗi kí tự nhận được\n",
    "def search(text:str,no_lower_than=0.2,num=3,num_result=10,topics=3):\n",
    "    top = search_to_topicid(text,no_lower_than,num)\n",
    "    results = search_results(top,num_result,topics)\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>categories</th>\n",
       "      <th>abstract</th>\n",
       "      <th>update_date</th>\n",
       "      <th>authors_parsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2406.04057</td>\n",
       "      <td>Overwhelmed Software Developers</td>\n",
       "      <td>10.1109/MS.2024.3374821</td>\n",
       "      <td>cs.SE cs.CY</td>\n",
       "      <td>We have conducted a qualitative psychology s...</td>\n",
       "      <td>2024-06-07</td>\n",
       "      <td>[['Michels', 'Lisa-Marie', ''], ['Petkova', 'A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2406.04066</td>\n",
       "      <td>Requirements for Organizational Resilience: En...</td>\n",
       "      <td>10.1109/MS.2024.3386035</td>\n",
       "      <td>cs.SE cs.CY</td>\n",
       "      <td>Can the right requirements boost developer s...</td>\n",
       "      <td>2024-06-07</td>\n",
       "      <td>[['Borg', 'Markus', ''], ['Graziotin', 'Daniel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2406.03859</td>\n",
       "      <td>From operculum and body tail movements to diff...</td>\n",
       "      <td>10.1016/j.compag.2020.105531</td>\n",
       "      <td>cs.CV q-bio.PE</td>\n",
       "      <td>The AEFishBIT tri-axial accelerometer was ex...</td>\n",
       "      <td>2024-06-07</td>\n",
       "      <td>[['Ferrer', 'Miguel A.', ''], ['Calduch-Giner'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2209.12222</td>\n",
       "      <td>Efficient Wrong-Way Risk Modelling for Funding...</td>\n",
       "      <td>10.1142/S0219024924500109</td>\n",
       "      <td>q-fin.CP q-fin.MF q-fin.RM</td>\n",
       "      <td>Wrong-Way Risk (WWR) is an important compone...</td>\n",
       "      <td>2024-06-07</td>\n",
       "      <td>[['van der Zwaard', 'T.', ''], ['Grzelak', 'L....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2406.035</td>\n",
       "      <td>Impact of aleatoric, stochastic and epistemic ...</td>\n",
       "      <td>10.1016/j.ijpe.2022.108626</td>\n",
       "      <td>stat.AP q-fin.RM</td>\n",
       "      <td>In construction projects, contingency reserv...</td>\n",
       "      <td>2024-06-07</td>\n",
       "      <td>[['Curto', 'David', ''], ['Acebes', 'Fernando'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2406.03547</td>\n",
       "      <td>Learning from the present for the future: the ...</td>\n",
       "      <td>10.1016/j.ascom.2024.100835</td>\n",
       "      <td>astro-ph.IM</td>\n",
       "      <td>The Forschungszentrum Juelich has been hosti...</td>\n",
       "      <td>2024-06-07</td>\n",
       "      <td>[['Manzano', 'C.', ''], ['Miskolczi', 'A.', ''...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>2305.0378</td>\n",
       "      <td>Boldness-Recalibration for Binary Event Predic...</td>\n",
       "      <td>10.1080/00031305.2024.2339266</td>\n",
       "      <td>stat.ME stat.ML</td>\n",
       "      <td>Probability predictions are essential to inf...</td>\n",
       "      <td>2024-06-07</td>\n",
       "      <td>[['Guthrie', 'Adeline P.', ''], ['Franck', 'Ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2401.04206</td>\n",
       "      <td>Effects of Multimodal Explanations for Autonom...</td>\n",
       "      <td>10.1038/s41598-024-62052-9</td>\n",
       "      <td>cs.HC cs.AI cs.RO</td>\n",
       "      <td>Advances in autonomous driving provide an op...</td>\n",
       "      <td>2024-06-07</td>\n",
       "      <td>[['Kaufman', 'Robert', ''], ['Costa', 'Jean', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>2305.10091</td>\n",
       "      <td>Multi-Agent Reinforcement Learning: Methods, A...</td>\n",
       "      <td>10.1109/TIV.2024.3408257</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>Multi-agent reinforcement learning (MARL) is...</td>\n",
       "      <td>2024-06-07</td>\n",
       "      <td>[['Zhou', 'Ziyuan', ''], ['Liu', 'Guanjun', ''...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>2311.13063</td>\n",
       "      <td>From Classification to Clinical Insights: Towa...</td>\n",
       "      <td>10.1145/3659604</td>\n",
       "      <td>cs.AI</td>\n",
       "      <td>Passively collected behavioral health data f...</td>\n",
       "      <td>2024-06-07</td>\n",
       "      <td>[['Englhardt', 'Zachary', ''], ['Ma', 'Chengqi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                              title  \\\n",
       "9   2406.04057                    Overwhelmed Software Developers   \n",
       "10  2406.04066  Requirements for Organizational Resilience: En...   \n",
       "27  2406.03859  From operculum and body tail movements to diff...   \n",
       "44  2209.12222  Efficient Wrong-Way Risk Modelling for Funding...   \n",
       "50    2406.035  Impact of aleatoric, stochastic and epistemic ...   \n",
       "52  2406.03547  Learning from the present for the future: the ...   \n",
       "62   2305.0378  Boldness-Recalibration for Binary Event Predic...   \n",
       "71  2401.04206  Effects of Multimodal Explanations for Autonom...   \n",
       "75  2305.10091  Multi-Agent Reinforcement Learning: Methods, A...   \n",
       "77  2311.13063  From Classification to Clinical Insights: Towa...   \n",
       "\n",
       "                              doi                  categories  \\\n",
       "9         10.1109/MS.2024.3374821                 cs.SE cs.CY   \n",
       "10        10.1109/MS.2024.3386035                 cs.SE cs.CY   \n",
       "27   10.1016/j.compag.2020.105531              cs.CV q-bio.PE   \n",
       "44      10.1142/S0219024924500109  q-fin.CP q-fin.MF q-fin.RM   \n",
       "50     10.1016/j.ijpe.2022.108626            stat.AP q-fin.RM   \n",
       "52    10.1016/j.ascom.2024.100835                 astro-ph.IM   \n",
       "62  10.1080/00031305.2024.2339266             stat.ME stat.ML   \n",
       "71     10.1038/s41598-024-62052-9           cs.HC cs.AI cs.RO   \n",
       "75       10.1109/TIV.2024.3408257                       cs.AI   \n",
       "77                10.1145/3659604                       cs.AI   \n",
       "\n",
       "                                             abstract update_date  \\\n",
       "9     We have conducted a qualitative psychology s...  2024-06-07   \n",
       "10    Can the right requirements boost developer s...  2024-06-07   \n",
       "27    The AEFishBIT tri-axial accelerometer was ex...  2024-06-07   \n",
       "44    Wrong-Way Risk (WWR) is an important compone...  2024-06-07   \n",
       "50    In construction projects, contingency reserv...  2024-06-07   \n",
       "52    The Forschungszentrum Juelich has been hosti...  2024-06-07   \n",
       "62    Probability predictions are essential to inf...  2024-06-07   \n",
       "71    Advances in autonomous driving provide an op...  2024-06-07   \n",
       "75    Multi-agent reinforcement learning (MARL) is...  2024-06-07   \n",
       "77    Passively collected behavioral health data f...  2024-06-07   \n",
       "\n",
       "                                       authors_parsed  \n",
       "9   [['Michels', 'Lisa-Marie', ''], ['Petkova', 'A...  \n",
       "10  [['Borg', 'Markus', ''], ['Graziotin', 'Daniel...  \n",
       "27  [['Ferrer', 'Miguel A.', ''], ['Calduch-Giner'...  \n",
       "44  [['van der Zwaard', 'T.', ''], ['Grzelak', 'L....  \n",
       "50  [['Curto', 'David', ''], ['Acebes', 'Fernando'...  \n",
       "52  [['Manzano', 'C.', ''], ['Miskolczi', 'A.', ''...  \n",
       "62  [['Guthrie', 'Adeline P.', ''], ['Franck', 'Ch...  \n",
       "71  [['Kaufman', 'Robert', ''], ['Costa', 'Jean', ...  \n",
       "75  [['Zhou', 'Ziyuan', ''], ['Liu', 'Guanjun', ''...  \n",
       "77  [['Englhardt', 'Zachary', ''], ['Ma', 'Chengqi...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vì một lý do nào đó, lúc chạy cái lệnh này thì với \"Investing\" thì vẫn có kết quả là chủ đề liên quan, nhưng với cái bản mới nhất thì lại không có\n",
    "search(\"investing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>categories</th>\n",
       "      <th>abstract</th>\n",
       "      <th>update_date</th>\n",
       "      <th>authors_parsed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2406.04057</td>\n",
       "      <td>Overwhelmed Software Developers</td>\n",
       "      <td>10.1109/MS.2024.3374821</td>\n",
       "      <td>cs.SE cs.CY</td>\n",
       "      <td>We have conducted a qualitative psychology s...</td>\n",
       "      <td>2024-06-07</td>\n",
       "      <td>[['Michels', 'Lisa-Marie', ''], ['Petkova', 'A...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                            title                      doi  \\\n",
       "9  2406.04057  Overwhelmed Software Developers  10.1109/MS.2024.3374821   \n",
       "\n",
       "    categories                                           abstract update_date  \\\n",
       "9  cs.SE cs.CY    We have conducted a qualitative psychology s...  2024-06-07   \n",
       "\n",
       "                                      authors_parsed  \n",
       "9  [['Michels', 'Lisa-Marie', ''], ['Petkova', 'A...  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_contents('2406.04057')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
